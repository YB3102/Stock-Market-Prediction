{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65e259b4-6220-4629-8f9d-636781862db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Information': 'Thank you for using Alpha Vantage! Our standard API rate limit is 25 requests per day. Please subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly remove all daily rate limits.'}\n",
      "Data for BA written to CSV.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 76\u001b[0m\n\u001b[1;32m     67\u001b[0m                             writer\u001b[38;5;241m.\u001b[39mwriterow([\n\u001b[1;32m     68\u001b[0m                                 ticker_symbol,\n\u001b[1;32m     69\u001b[0m                                 sentiment_score,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m                                 relevance_score\n\u001b[1;32m     74\u001b[0m                             ])\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgood\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m written to CSV.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# To respect API rate limits\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError fetching data for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgood\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "import time\n",
    "\n",
    "renata_api_key = 'ONA3WDTTO41MMX61'\n",
    "akhil_api_key = 'H75ECN1EJAKT0STP'\n",
    "yatharth_api_key = 'RGETEZDQFE1284PD'\n",
    "new_api_key = 'WOLZ6SVL0KILLCIC'\n",
    "base_url = 'https://www.alphavantage.co/query'\n",
    "\n",
    "consumer_goods = [\"BA\", \"ADBE\", \"AMZN\", \"MRNA\"]\n",
    "financial_tickers = [ \"PYPL\", \"BAC\", \"CI\", \"GS\"]\n",
    "tech_ticker = [\"TSLA\", \"NVDA\", \"INTC\", \"AMD\"]\n",
    "csv_name = 'news_sentiment_data.csv'\n",
    "topics = \"mergers_and_acquisitions, earnings , financial_markets, economy_fiscal , economy_monetary\"\n",
    "headers = ['Ticker', 'Sentiment Score','Sentiment Label','Published Date', 'Topic','Relevance Score' ]\n",
    "\n",
    "with open(csv_name, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(headers)\n",
    "\n",
    "#iterate through consumer goods tickers\n",
    "for good in consumer_goods:\n",
    "    params = {\n",
    "        'function': 'NEWS_SENTIMENT',\n",
    "        'tickers': good,\n",
    "        'topics' : topics,\n",
    "        'time_from': '0230101T0000',\n",
    "        'sort' : 'RELEVANCE', \n",
    "        'limit' : 1000,\n",
    "        'apikey': new_api_key\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params = params)\n",
    "        data = response.json()\n",
    "        print(data) \n",
    "        if 'feed' in data:\n",
    "            print(\"feed is not empty\")\n",
    "            for news_item in data['feed']:\n",
    "                # Extract the published date\n",
    "                published_date = news_item.get('time_published', 'N/A')\n",
    "\n",
    "                # Extract topics and relevance score from the 'topics' list\n",
    "                topic_data = news_item.get('topics', [])\n",
    "                if topic_data:\n",
    "                    topic = topic_data[0].get('topic', 'N/A')\n",
    "                    relevance_score = topic_data[0].get('relevance_score', 'N/A')\n",
    "                else:\n",
    "                    topic = 'N/A'\n",
    "                    relevance_score = 'N/A'\n",
    "\n",
    "                # Extract ticker sentiment details\n",
    "                ticker_sentiment_data = news_item.get('ticker_sentiment', [])\n",
    "                if ticker_sentiment_data:\n",
    "                    for sentiment in ticker_sentiment_data:\n",
    "                        # Only include data for the current ticker in the loop\n",
    "                        if sentiment.get('ticker') == ticker:\n",
    "                            ticker_symbol = sentiment.get('ticker', 'N/A')\n",
    "                            sentiment_score = sentiment.get('ticker_sentiment_score', 'N/A')\n",
    "                            sentiment_label = sentiment.get('ticker_sentiment_label', 'N/A')\n",
    "\n",
    "                            # Write filtered data to CSV\n",
    "                            with open(output_csv, mode='a', newline='', encoding='utf-8') as file:\n",
    "                                writer = csv.writer(file)\n",
    "                                writer.writerow([\n",
    "                                    ticker_symbol,\n",
    "                                    sentiment_score,\n",
    "                                    sentiment_label,\n",
    "                                    published_date,\n",
    "                                    topic,\n",
    "                                    relevance_score\n",
    "                                ])\n",
    "        print(f\"Data for {good} written to CSV.\")\n",
    "        time.sleep(15)  # To respect API rate limits\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {good}: {e}\")\n",
    "\n",
    "print(\"Data collection complete.\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f5d3f3-0946-4ff3-a415-b9cab5009808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030135f3-6e5a-4f5d-8ba0-a514e3b849fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
